# -*- coding: utf-8 -*-
"""aad-copy-utk-code-bci-4-2a-aad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aJjcAzMVbRBGp5Yo7RlzaCsa66-yd6Ty

## how to improve acc

- overlapping window epochs  to inc data size
    - currently 500 ms of one epoch
      
- class weights if imbalanced

---> baseline correction

---> re ref

## STEP 1) BCI 4 2a data TRAIM
- using utk lastest code after grid search
"""



"""### trans learn -  A 1)
A- initially trained on BCI 4 2a
B- initially trained on physio

1- only dense retrained
2 - dense + conv retrained
3- full model
"""



# main_grid_search.py (Loading GDF, Using Multiple Models - CORRECTED)

# --- Essential Imports ---
import os
import numpy as np
import tensorflow as tf
import mne # For loading GDF and processing
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import itertools
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D, SeparableConv2D,
                                     BatchNormalization, Activation, AveragePooling2D,
                                     Dropout, Flatten, Dense, Concatenate)
from tensorflow.keras.constraints import max_norm # Re-added for EEGNet_MSD
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.optimizers.schedules import LearningRateSchedule
from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, EarlyStopping # Added EarlyStopping
import time
import logging
import math
import traceback
# No scipy.io needed if loading GDF directly

# --- Import Model Building Functions ---
    # Assuming eeg_models.py contains the necessary functions
    ##from eeg_models import (build_eegnet, build_eegnet_mha, build_eegnet_msd,  build_eegnet_msd_mha, build_q_eegnet, build_q_eegnet_mha)


# --- Configuration ---
GDF_FILE_PATH ="/kaggle/input/eeg-bci-4-2a/A01T.gdf" # "/content/drive/MyDrive/eye eye tea Dee/data - bci 4 2a/A01T.gdf"   #"/home/scai/mtech/aib242287/COL761/Assignment-3/dataset/A01T.gdf" # Source GDF file - PLEASE VERIFY THIS PATH
#BASE_LOG_DIR = "eeg_multi_model_gdf_grid_search_logs" # Updated Log dir name

# Event mapping for Left (769) vs Right (770) hand
EVENT_ID_MAPPING = {'769': 7, '770': 8} # Using distinct integers > standard mne codes
INTERNAL_LABEL_MAP = {v: i for i, v in enumerate(sorted(EVENT_ID_MAPPING.values()))} # Maps 7->0, 8->1
NUM_CLASSES = len(EVENT_ID_MAPPING)
TARGET_NAMES = ['Left Hand (769)', 'Right Hand (770)']

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# --- Grid Search Parameter Lists ---
# Define which models to include in the grid search
def build_q_eegnet(input_shape, num_classes=2, dropout_rate=0.5, l2_rate=1e-4, **kwargs):
    """Placeholder 'Quantized-like' EEGNet (Lighter variant for demonstration)."""
    n_channels, n_times, _ = input_shape
    inputs = Input(shape=input_shape, name='Input')
    # Block 1 - Reduced Filters/Depth Multiplier
    F1 = 8 # Reduced from 16 in build_eegnet
    D = 1 # Reduced from 2
    kernLength = 51 # Keep same as build_eegnet
    x = Conv2D(F1, (1, kernLength), padding='same', use_bias=False, kernel_regularizer=l2(l2_rate))(inputs)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((n_channels, 1), depth_multiplier=D, use_bias=False, depthwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 4))(x)
    x = Dropout(dropout_rate)(x)
    # Block 2 - Reduced Filters
    F2 = 16 # Reduced from 32
    kernLength2 = 15 # Keep same as build_eegnet
    x = SeparableConv2D(F2, (1, kernLength2), padding='same', use_bias=False, depthwise_regularizer=l2(l2_rate), pointwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 8))(x)
    x = Dropout(dropout_rate)(x)
    # Classification Head - Reduced Dense Layer
    x = Flatten()(x)
    x = Dense(32, activation='elu', kernel_regularizer=l2(l2_rate))(x) # Reduced from 64
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs, name='build_q_eegnet')

model_builders = {
    # 'EEGNet': build_eegnet,
    # 'EEGNetMHA': build_eegnet_mha,
    # 'EEGNetMSD': build_eegnet_msd,
    # 'EEGNetMSDMHA': build_eegnet_msd_mha,
    'QEECNet': build_q_eegnet, # Reminder: Lighter variant, not true quantization
    # 'QEECNetMHA': build_q_eegnet_mha # Reminder: Lighter variant, not true quantization
}


# Hyperparameters (Expand these lists for a wider search)

optimizers_list = ['adam'] # Example: added adam
max_norm_vals_list = [1.0]    # Example: added another max norm
num_heads_list = [4]             # Relevant for MHA models
lrs_list = [0.001]
dropouts_list = [0.25]
batch_sizes_list = [ 64]
schedulers_list = ['clr']
l2_rates_list = [0.0001] # Renamed from reg_rates_list
low_freq_list = [5.0]
high_freq_list = [35.0]
tmin_list = [0.0]
tmax_list = [3.0]

#--UTKRSH OPTIMAL AFTER GRID SEARCH
#Parameters: {'model': 'EEGNet', 'opt': 'adam', 'lr': 0.001, 'dropout': 0.25, 'batch': 64, 'sched': 'clr', 'l2': 0.0001, 'max_norm': 1.0, 'num_heads': 4, 'low_f': 5.0, 'high_f': 35.0, 'tmin': 0.0, 'tmax': 3.0}


# --- Training Configuration ---
EPOCHS_GRID = 300 # Fixed number of epochs

EARLY_STOPPING_PATIENCE = 50 # Patience for EarlyStopping

# --- Model Saving Configuration ---
PERIODIC_SAVE_START_EPOCH = 300  # Start saving periodically after epoch 50
PERIODIC_SAVE_MIN_ACC = 0.85 # Min val acc to save periodically
PERIODIC_SAVE_FREQ = 10      # Save every 10 epochs (if conditions met)
print("cell -1")

# --- Custom Logging Callback ---
class EpochLogger(Callback):
    """Logs training/validation accuracy, loss, and LR at the end of each epoch."""
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        current_lr = self.model.optimizer.learning_rate
        lr_val = -1.0
        try:
            if isinstance(current_lr, tf.keras.optimizers.schedules.LearningRateSchedule):
                # Use iterations which increments correctly per step (batch)
                step = self.model.optimizer.iterations
                lr_val = current_lr(step).numpy()
            elif hasattr(current_lr, 'numpy'): # Handle tf.Variable case
                lr_val = current_lr.numpy()
            else: # Handle float case
                lr_val = current_lr # Already a float
        except Exception as e:
            # logging.debug(f"Could not fetch LR: {e}") # Optional debug log
            pass # Ignore LR fetch errors during logging if needed
        print(f"Epoch {epoch+1:03d}/{self.params['epochs']}: TrAcc={logs.get('accuracy', -1):.4f}, ValAcc={logs.get('val_accuracy', -1):.4f}, TrLoss={logs.get('loss', -1):.4f}, ValLoss={logs.get('val_loss', -1):.4f}, LR={lr_val:.6f}")



# --- Data Preprocessing Function (Using GDF Loader Workaround) ---
def load_and_preprocess_gdf(gdf_file_path, low_freq, high_freq, event_id_mapping):
    """Loads GDF, applies filtering based on parameters, using a two-step load."""
    if not os.path.exists(gdf_file_path):
        raise FileNotFoundError(f"GDF file not found: {gdf_file_path}")
    raw = None
    try:
        logging.info(f"Loading GDF header/annotations: {gdf_file_path}...")
        # Load annotations first, but not data yet
        raw = mne.io.read_raw_gdf(gdf_file_path, preload=False, verbose='warning')
        logging.info("Header and annotations loaded.")
        sfreq = raw.info['sfreq']

        logging.info(f"Extracting events from annotations using mapping: {event_id_mapping}...")
        # Use the user-provided mapping { '769': 7, '770': 8 }
        events, event_dict_mne_internal = mne.events_from_annotations(raw, event_id=event_id_mapping, verbose='warning')
        # event_dict_mne_internal will likely be {'Left Hand (769)': 7, 'Right Hand (770)': 8}
        logging.info(f"Found {len(events)} events corresponding to specified annotations.")
        if len(events) == 0:
            raise ValueError("No relevant events found via annotations using the provided mapping.")

        logging.info("Loading GDF data into memory...")
        raw.load_data(verbose='warning')
        logging.info("Data loaded into memory.")

        # Pick only EEG channels (first 22 as per PDF description)
        try:
            # Explicitly pick the first 22 channels if they represent EEG
            num_eeg_channels = 22
            if len(raw.ch_names) >= num_eeg_channels:
                 picks = raw.ch_names[:num_eeg_channels]
                 raw.pick(picks=picks)
                 logging.info(f"Selected the first {num_eeg_channels} channels assumed to be EEG: {raw.ch_names}")
            else:
                 logging.warning(f"Found fewer than {num_eeg_channels} channels. Using all {len(raw.ch_names)} channels.")
                 raw.pick(picks='eeg', errors='ignore') # Fallback to picking by type 'eeg' if names aren't reliable

            # Exclude EOG (usually last 3 channels in this dataset)
            raw.drop_channels([ch for ch in raw.ch_names if 'EOG' in ch.upper()], on_missing='ignore')
            logging.info(f"Channels after picking EEG and dropping EOG (if found): {len(raw.ch_names)}")

        except Exception as e:
            logging.warning(f"Error picking/dropping channels: {e}. Proceeding with available channels.")

        logging.info(f"Applying band-pass filter ({low_freq:.1f}-{high_freq:.1f} Hz)...")
        raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge', verbose='warning')

        # Pass the original event_id_mapping { '769': 7, '770': 8 } for epoching
        return raw, events, sfreq, event_id_mapping

    except ValueError as e:
        logging.error(f"Event extraction error: {e}.")
        if raw:
             logging.error(f"Available annotation descriptions: {np.unique(raw.annotations.description)}")
        return None, None, 0, None
    except Exception as e:
        logging.error(f"Error during GDF loading/preprocessing: {e}", exc_info=True)
        return None, None, 0, None

print("cell -2")

# --- Data Preparation Function ---
def prepare_data_from_gdf(raw, events, sfreq, tmin, tmax, event_id_map_for_epochs, internal_label_map):
    """Creates epochs, applies artifact rejection, and prepares data arrays."""
    if raw is None or events is None or sfreq <= 0: return None, None, 0

    duration = tmax - tmin
    expected_samples = int(math.ceil(duration * sfreq))
    logging.info(f"Creating epochs ({tmin:.1f}s to {tmax:.1f}s relative to events -> {expected_samples} samples)...")
    try:
        # Use the { '769': 7, '770': 8 } mapping here for creating epochs
        epochs = mne.Epochs(raw, events, event_id=event_id_map_for_epochs, tmin=tmin, tmax=tmax,
                           baseline=None, # No baseline correction in this window
                           preload=True, verbose='warning')
        logging.info(f"Initial epochs created: {len(epochs)}")
        if len(epochs) == 0: raise ValueError("Epoch object empty after creation.")

        # --- ADDED: Basic Artifact Rejection ---
        original_num_epochs = len(epochs)
        # Simple peak-to-peak rejection - adjust threshold as needed
        reject_criteria = dict(eeg=150e-6) # Reject epochs where any EEG channel exceeds 150 uV peak-to-peak
        try:
            epochs.drop_bad(reject=reject_criteria, verbose='warning')
            logging.info(f"Epochs after PTP rejection (> {reject_criteria['eeg']*1e6:.0f} uV): {len(epochs)} (dropped {original_num_epochs - len(epochs)})")
        except Exception as reject_err:
            logging.warning(f"Could not apply artifact rejection: {reject_err}. Using all created epochs.")

        if len(epochs) == 0:
            logging.error("No epochs remaining after artifact rejection.")
            return None, None, 0
        # --- End Artifact Rejection ---

    except Exception as e:
        logging.error(f"Epoch creation or rejection error: {e}")
        return None, None, 0

    actual_samples_per_epoch = epochs.get_data().shape[-1]
    if actual_samples_per_epoch != expected_samples:
        logging.warning(f"Actual samples ({actual_samples_per_epoch}) differs from expected ({expected_samples}). Using actual.")

    X = epochs.get_data(units='uV') # Get data in microvolts
    logging.info(f"Epoch data shape: {X.shape}")

    # Map the original MNE event IDs (7, 8) to internal labels (0, 1)
    y_integers = epochs.events[:, -1] # Get the event IDs (e.g., 7, 8)
    try:
        y = np.array([internal_label_map[event_id] for event_id in y_integers])
    except KeyError as e:
        logging.error(f"Label mapping error for event ID {e}. Check INTERNAL_LABEL_MAP: {internal_label_map}")
        return None, None, 0

    logging.info(f"Labels shape: {y.shape}, Unique internal labels: {np.unique(y)}, Distribution: {np.bincount(y)}")

    # Reshape for Conv2D input (add channel dimension)
    X = X[..., np.newaxis]
    logging.info(f"Reshaped X shape for model: {X.shape}")

    return X, y, actual_samples_per_epoch
print("cell 3")

class CustomCyclicLR(tf.keras.optimizers.schedules.LearningRateSchedule):
    """Custom Cyclic Learning Rate Schedule (Triangular)."""
    def __init__(self, base_lr, max_lr, step_size, name="CustomCyclicLR"):
        super().__init__()
        self.base_lr = tf.cast(base_lr, tf.float32)
        self.max_lr = tf.cast(max_lr, tf.float32)
        self.step_size = tf.cast(step_size, tf.float32)
        self.step_counter = tf.Variable(0., trainable=False, dtype=tf.float32, name="step_counter")
        self.name = name
    def __call__(self, step=None):
        # Use optimizer's iterations if available, otherwise internal counter
        current_step = tf.cast(step, tf.float32) if step is not None else self.step_counter
        if step is None:
            self.step_counter.assign_add(1.0)

        safe_step_size = tf.maximum(self.step_size, 1e-9)
        cycle = tf.floor(1 + current_step / (2 * safe_step_size))
        x = tf.abs(current_step / safe_step_size - 2 * cycle + 1)
        lr = self.base_lr + (self.max_lr - self.base_lr) * tf.maximum(0., (1 - x))
        return lr
    def get_config(self):
        return { "base_lr": float(self.base_lr.numpy()) if hasattr(self.base_lr, 'numpy') else float(self.base_lr),
                 "max_lr": float(self.max_lr.numpy()) if hasattr(self.max_lr, 'numpy') else float(self.max_lr),
                 "step_size": float(self.step_size.numpy()) if hasattr(self.step_size, 'numpy') else float(self.step_size),
                 "name": self.name }
model = 'EEGNet'
opt = 'adam'
lr = 0.001
dropout = 0.25
batch = 64
sched = 'clr'
l2r = 0.0001
max_norm = 1.0
num_heads = 4
low_f = 5.0
high_f = 35.0
tmin = 0.0
tmax = 3.0



#print(f"--- Starting Grid Search ---")

    # --- Combination Setup ---
combo_start_time = time.time()
    # Collect parameters used in this specific combination
    ##combo_params = { 'model': model_name, 'opt': opt_name, 'lr': lr, 'dropout': drop, 'batch': batch,'sched': sched, 'l2': l2r, 'max_norm': mn, 'num_heads': num_h,      'low_f': low_f, 'high_f': high_f, 'tmin': t_min, 'tmax': t_max }
    ##---> utk optimal params
    #combo_params: {'model': 'EEGNet', 'opt': 'adam', 'lr': 0.001, 'dropout': 0.25, 'batch': 64, 'sched': 'clr', 'l2': 0.0001, 'max_norm': 1.0, 'num_heads': 4, 'low_f': 5.0, 'high_f': 35.0, 'tmin': 0.0, 'tmax': 3.0}

print("cell 4")

# --- Data Preparation ---
# Pass the original EVENT_ID_MAPPING { '769': 7, '770': 8 }
raw_obj, events_array, sfreq, event_map_for_epochs = load_and_preprocess_gdf(
    GDF_FILE_PATH, low_f, high_f, EVENT_ID_MAPPING
)

"""## EPOCHING"""

#-------------------------------------->>> EPOCH DATA STRUCTURE  change her onwards if diff epoching str START_------------------------

if raw_obj is None:
    print("Data loading/preprocessing failed. Skipping combination.")


# Pass event_map_for_epochs ({ '769': 7, '770': 8 }) and internal_label_map ({7:0, 8:1})
X_data, y_data, actual_samples = prepare_data_from_gdf(
#    raw_obj, events_array, sfreq, t_min, t_max, event_map_for_epochs, INTERNAL_LABEL_MAP
    raw_obj, events_array, sfreq, 0, 3, event_map_for_epochs, INTERNAL_LABEL_MAP
)
#del raw_obj, events_array # Free memory
if X_data is None or actual_samples == 0:
    print("Epoch extraction or artifact rejection failed. Skipping combination.")


# Convert labels to categorical format for TF/Keras
y_data_cat = tf.keras.utils.to_categorical(y_data, num_classes=NUM_CLASSES)

# --- Split data ---

X_tr_c, X_val_c, y_tr_c, y_val_c = train_test_split(
        X_data, y_data_cat, test_size=0.2, random_state=42, stratify=y_data # Stratify by original integer labels
    )

#-------------------------------------->>> EPOCH DATA STRUCTURE  change her onwards if diff epoching str ENDS------------------------

del X_data, y_data, y_data_cat # Free memory

#------------------> to save model with val acc

import os
import tensorflow as tf
from tensorflow.keras.callbacks import Callback

class SaveBestModelWithValAcc(Callback):
    def __init__(self, save_dir='models', prefix='best_model'):
        super().__init__()
        self.best_val_acc = -1
        self.save_dir = save_dir
        self.prefix = prefix
        os.makedirs(save_dir, exist_ok=True)

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        val_acc = logs.get('val_accuracy')
        if val_acc is not None and val_acc > self.best_val_acc:
            self.best_val_acc = val_acc
            filename = f"{self.prefix}_valacc_{val_acc:.4f}.keras"
            filepath = os.path.join(self.save_dir, filename)
            self.model.save(filepath)
            print(f"✅ Saved new best model to {filepath}")

callbacks = [
    EpochLogger(),
    SaveBestModelWithValAcc(save_dir='saved_models', prefix='EEGNet')
]



if opt.lower() == 'adam': optimizer = Adam(learning_rate=l2r)
optimizer = Adam(learning_rate=l2r)
# --- Model & Training Setup ---
tf.keras.backend.clear_session() # Clear previous models from memory

if sched == 'clr':
        # Calculate steps per epoch based on training data size and batch size
        steps_per_epoch = math.ceil(len(X_tr_c) / batch)
        if steps_per_epoch <= 0: steps_per_epoch = 1 # Avoid division by zero if data is tiny
        epochs_per_half_cycle = 8 # How many epochs for one ascent/descent
        clr_step_size = steps_per_epoch * epochs_per_half_cycle
        clr_base_lr = lr / 10.0 # Base LR is often 1/10th of max LR
        lr_to_use = CustomCyclicLR(base_lr=clr_base_lr, max_lr=lr, step_size=clr_step_size)
        logging.info(f"Using Custom Cyclic LR: base={clr_base_lr:.1e}, max={lr:.1e}, step_size={int(clr_step_size)} steps.")

current_input_shape = X_tr_c.shape[1:] # Shape is (Channels, Samples, 1)
        # Call the function stored in model_builder (e.g., build_eegnet_msd)
        # Pass necessary parameters, kwargs catch extras like num_heads for models that don't use them

#----
model_name = 'q_eegnet'
model = build_q_eegnet( #model_builder(
            input_shape=current_input_shape,
            num_classes=NUM_CLASSES,
            dropout_rate=dropout,
            l2_rate=l2r,
            max_norm_val=max_norm, # Pass max_norm (used by MSD models)
            num_heads=num_heads    # Pass num_heads (used by MHA models)
        )

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

'''
    # --- Callbacks ---
#callbacks = [EpochLogger()] # Log epoch details
#timing_callback = TimingCallback()
#callbacks.append(timing_callback) # Log timing


from tensorflow.keras.callbacks import ModelCheckpoint
checkpoint_cb = ModelCheckpoint(
    filepath='aad_best_model_valacc.keras',     # You can change the filename or add a path
    monitor='val_accuracy',              # What to monitor
    save_best_only=True,                 # Only save the best model
    mode='max',                          # Because higher val_accuracy is better
    verbose=1                            # Show when model is saved
)
callbacks = [EpochLogger(), checkpoint_cb]
'''

X_tr_c[0:1].shape

sample_input = X_tr_c[0:1]  # Select the first sample with batch size 1

X_tr_c.shape

"""## time computation - model select"""

from tensorflow.keras.constraints import max_norm
def build_eegnet(input_shape, num_classes=2, dropout_rate=0.5, l2_rate=1e-4, **kwargs):
    """User provided EEGNet variant. Accepts extra kwargs to ignore unused grid params."""
    n_channels, n_times, _ = input_shape
    logging.debug(f"Building 'build_eegnet' with Input Shape: {input_shape}, Num Classes: {num_classes}, Dropout: {dropout_rate}, L2 Rate: {l2_rate}")
    inputs = Input(shape=input_shape, name='Input')
    # Block 1
    x = Conv2D(16, (1, 51), padding='same', use_bias=False, kernel_regularizer=l2(l2_rate))(inputs)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((n_channels, 1), depth_multiplier=2, use_bias=False, depthwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 4))(x)
    x = Dropout(dropout_rate)(x)
    # Block 2
    x = SeparableConv2D(32, (1, 15), padding='same', use_bias=False, depthwise_regularizer=l2(l2_rate), pointwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 8))(x)
    x = Dropout(dropout_rate)(x)
    # Classification Head
    x = Flatten()(x)
    x = Dense(64, activation='elu', kernel_regularizer=l2(l2_rate))(x)
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs, name='build_eegnet')

def build_eegnet_mha(input_shape, num_classes=2, dropout_rate=0.5, l2_rate=1e-4, num_heads=4, **kwargs):
    """EEGNet variant incorporating Multi-Head Self-Attention blocks. Accepts num_heads."""
    n_channels, n_times, _ = input_shape
    logging.debug(f"Building 'build_eegnet_mha' - Input: {input_shape}, Classes: {num_classes}, Dropout: {dropout_rate}, L2: {l2_rate}, Heads: {num_heads}")
    inputs = Input(shape=input_shape, name='Input')

    # Block 1 Base (Conv -> BN -> DWConv -> BN -> Act)
    x = Conv2D(16, (1, 51), padding='same', use_bias=False, kernel_regularizer=l2(l2_rate))(inputs)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((n_channels, 1), depth_multiplier=2, use_bias=False, depthwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x_act1 = Activation('elu')(x) # Output shape: (batch, Chans, Samples, 16*2=32)

    # --- Attention 1 ---
    shape_b1 = tf.keras.backend.int_shape(x_act1)
    permuted_b1 = Permute((2, 1, 3))(x_act1) # (batch, Samples, Chans, 32)
    features_dim1 = shape_b1[1] * shape_b1[3] # Chans * 32
    reshaped_attn1_input = Reshape((-1, features_dim1))(permuted_b1) # (batch, Samples, Features)
    key_dim1 = max(1, features_dim1 // num_heads)
    logging.debug(f"  MHA 1: Input={reshaped_attn1_input.shape}, Heads={num_heads}, KeyDim={key_dim1}")
    attn1_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim1)(query=reshaped_attn1_input, value=reshaped_attn1_input, key=reshaped_attn1_input)
    attn1_output = LayerNormalization()(attn1_output + reshaped_attn1_input) # Add & Norm
    reshaped_attn1_output = Reshape((shape_b1[2], shape_b1[1], shape_b1[3]))(attn1_output)
    permuted_attn1_output = Permute((2, 1, 3))(reshaped_attn1_output) # Back to (batch, Chans, Samples, 32)

    # Continue Block 1
    x = AveragePooling2D((1, 4))(permuted_attn1_output) # (batch, Chans, Samples/4, 32)
    x = Dropout(dropout_rate)(x)

    # Block 2 Base (SeparableConv -> BN -> Act)
    x = SeparableConv2D(32, (1, 15), padding='same', use_bias=False, depthwise_regularizer=l2(l2_rate), pointwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x_act2 = Activation('elu')(x) # Output shape: (batch, Chans, Samples/4, 32)

    # --- Attention 2 ---
    shape_b2 = tf.keras.backend.int_shape(x_act2)
    permuted_b2 = Permute((2, 1, 3))(x_act2) # (batch, Samples/4, Chans, 32)
    features_dim2 = shape_b2[1] * shape_b2[3] # Chans * 32
    reshaped_attn2_input = Reshape((-1, features_dim2))(permuted_b2) # (batch, Samples/4, Features)
    key_dim2 = max(1, features_dim2 // num_heads)
    logging.debug(f"  MHA 2: Input={reshaped_attn2_input.shape}, Heads={num_heads}, KeyDim={key_dim2}")
    attn2_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim2)(query=reshaped_attn2_input, value=reshaped_attn2_input, key=reshaped_attn2_input)
    attn2_output = LayerNormalization()(attn2_output + reshaped_attn2_input) # Add & Norm
    reshaped_attn2_output = Reshape((shape_b2[2], shape_b2[1], shape_b2[3]))(attn2_output)
    permuted_attn2_output = Permute((2, 1, 3))(reshaped_attn2_output) # Back to (batch, Chans, Samples/4, 32)

    # Continue Block 2
    x = AveragePooling2D((1, 8))(permuted_attn2_output) # (batch, Chans, Samples/32, 32)
    x = Dropout(dropout_rate)(x)

    # Classification Head
    x = Flatten()(x)
    x = Dense(64, activation='elu', kernel_regularizer=l2(l2_rate))(x)
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs, name='build_eegnet_mha')

def build_eegnet_msd_mha(input_shape, num_classes=2, dropout_rate=0.5, l2_rate=1e-4, max_norm_val=1.0, num_heads=4, **kwargs):
    """EEGNet-MSD model incorporating Multi-Head Self-Attention blocks."""
    n_channels, n_times, _ = input_shape
    logging.debug(f"Building 'build_eegnet_msd_mha' - Input: {input_shape}, Classes: {num_classes}, Dropout: {dropout_rate}, L2: {l2_rate}, MaxNorm: {max_norm_val}, Heads: {num_heads}")
    inputs = Input(shape=input_shape, name='Input')

    # Default MSD parameters
    kernel_sizes = kwargs.get('kernel_sizes', [16, 32, 64])
    F1 = kwargs.get('F1', 16)
    D = kwargs.get('D', 2)
    F2 = kwargs.get('F2', 64)

    multi_scale_blocks = []
    # Multi-Scale Temporal Block
    for k_len in kernel_sizes:
        branch_name = f'branch_k{k_len}'
        branch = Conv2D(F1, (1, k_len), padding='same', use_bias=False, kernel_regularizer=l2(l2_rate), name=f'{branch_name}_conv1')(inputs)
        branch = BatchNormalization(name=f'{branch_name}_bn1')(branch)
        branch = DepthwiseConv2D((n_channels, 1), depth_multiplier=D, use_bias=False, depthwise_constraint=max_norm(max_norm_val), depthwise_regularizer=l2(l2_rate), name=f'{branch_name}_dwconv')(branch)
        branch = BatchNormalization(name=f'{branch_name}_bn2')(branch)
        branch = Activation('elu', name=f'{branch_name}_act1')(branch)
        # --- No Attention within branches, pool first ---
        branch = AveragePooling2D((1, 4), name=f'{branch_name}_pool1')(branch)
        branch = Dropout(dropout_rate, name=f'{branch_name}_drop1')(branch)
        multi_scale_blocks.append(branch)

    merged = Concatenate(axis=-1, name='concatenate_branches')(multi_scale_blocks)
    # Output shape: (batch, Chans, Samples/4, F1*D*len(kernel_sizes))

    # --- Attention 1 (Applied after merging multi-scale features) ---
    shape_m1 = tf.keras.backend.int_shape(merged)
    permuted_m1 = Permute((2, 1, 3))(merged) # (batch, Samples/4, Chans, Feats)
    features_dim_m1 = shape_m1[1] * shape_m1[3] # Chans * (F1*D*len(kernel_sizes))
    reshaped_attn_m1_input = Reshape((-1, features_dim_m1))(permuted_m1) # (batch, Samples/4, Features)
    key_dim_m1 = max(1, features_dim_m1 // num_heads)
    logging.debug(f"  MSD MHA 1: Input={reshaped_attn_m1_input.shape}, Heads={num_heads}, KeyDim={key_dim_m1}")
    attn_m1_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim_m1)(query=reshaped_attn_m1_input, value=reshaped_attn_m1_input, key=reshaped_attn_m1_input)
    attn_m1_output = LayerNormalization()(attn_m1_output + reshaped_attn_m1_input) # Add & Norm
    reshaped_attn_m1_output = Reshape((shape_m1[2], shape_m1[1], shape_m1[3]))(attn_m1_output)
    permuted_attn_m1_output = Permute((2, 1, 3))(reshaped_attn_m1_output) # Back to (batch, Chans, Samples/4, Feats)

    # Separable Convolution Block
    x = SeparableConv2D(F2, (1, 16), use_bias=False, padding='same', depthwise_regularizer=l2(l2_rate), pointwise_regularizer=l2(l2_rate), name='sepconv1')(permuted_attn_m1_output)
    x = BatchNormalization(name='bn3')(x)
    x_act2 = Activation('elu', name='act2')(x)
    # Output shape: (batch, Chans, Samples/4, F2)

    # --- Attention 2 ---
    shape_b2 = tf.keras.backend.int_shape(x_act2)
    permuted_b2 = Permute((2, 1, 3))(x_act2)
    features_dim2 = shape_b2[1] * shape_b2[3]
    reshaped_attn2_input = Reshape((-1, features_dim2))(permuted_b2)
    key_dim2 = max(1, features_dim2 // num_heads)
    logging.debug(f"  MSD MHA 2: Input={reshaped_attn2_input.shape}, Heads={num_heads}, KeyDim={key_dim2}")
    attn2_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim2)(query=reshaped_attn2_input, value=reshaped_attn2_input, key=reshaped_attn2_input)
    attn2_output = LayerNormalization()(attn2_output + reshaped_attn2_input)
    reshaped_attn2_output = Reshape((shape_b2[2], shape_b2[1], shape_b2[3]))(attn2_output)
    permuted_attn2_output = Permute((2, 1, 3))(reshaped_attn2_output)

    # Continue Block 2
    x = AveragePooling2D((1, 8), name='pool2')(permuted_attn2_output)
    x = Dropout(dropout_rate, name='drop2')(x)

    # Additional Conv Block
    x = Conv2D(F2 * 2, (1, 16), use_bias=False, padding='same', kernel_regularizer=l2(l2_rate), name='conv2')(x)
    x = BatchNormalization(name='bn4')(x)
    x = Activation('elu', name='act3')(x)
    x = AveragePooling2D((1, 4), name='pool3')(x)
    x = Dropout(dropout_rate, name='drop3')(x)

    # Classification Head
    x = Flatten(name='flatten')(x)
    dense = Dense(num_classes, kernel_constraint=max_norm(max_norm_val), kernel_regularizer=l2(l2_rate), name='dense_output')(x)
    outputs = Activation('softmax', name='softmax')(dense)
    return Model(inputs, outputs, name='build_eegnet_msd_mha')

# --- Model 6: Q-EEGNet with Multi-Head Attention ---
def build_q_eegnet_mha(input_shape, num_classes=2, dropout_rate=0.5, l2_rate=1e-4, num_heads=4, **kwargs):
    """Placeholder 'Quantized-like' EEGNet (Lighter Variant) with MHA blocks."""
    n_channels, n_times, _ = input_shape
    logging.debug(f"Building 'build_q_eegnet_mha' (Lighter Variant) - Input: {input_shape}, Classes: {num_classes}, Dropout: {dropout_rate}, L2: {l2_rate}, Heads: {num_heads}")
    inputs = Input(shape=input_shape, name='Input')

    # Block 1 Base (Lighter)
    F1 = 8; D = 1; kernLength = 51
    x = Conv2D(F1, (1, kernLength), padding='same', use_bias=False, kernel_regularizer=l2(l2_rate))(inputs)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((n_channels, 1), depth_multiplier=D, use_bias=False, depthwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x_act1 = Activation('elu')(x) # Output shape: (batch, Chans, Samples, F1*D=8)

    # Attention 1
    shape_b1 = tf.keras.backend.int_shape(x_act1)
    permuted_b1 = Permute((2, 1, 3))(x_act1)
    features_dim1 = shape_b1[1] * shape_b1[3] # Chans * 8
    reshaped_attn1_input = Reshape((-1, features_dim1))(permuted_b1)
    key_dim1 = max(1, features_dim1 // num_heads)
    logging.debug(f"  Q MHA 1: Input={reshaped_attn1_input.shape}, Heads={num_heads}, KeyDim={key_dim1}")
    attn1_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim1)(query=reshaped_attn1_input, value=reshaped_attn1_input, key=reshaped_attn1_input)
    attn1_output = LayerNormalization()(attn1_output + reshaped_attn1_input)
    reshaped_attn1_output = Reshape((shape_b1[2], shape_b1[1], shape_b1[3]))(attn1_output)
    permuted_attn1_output = Permute((2, 1, 3))(reshaped_attn1_output)

    # Continue Block 1
    x = AveragePooling2D((1, 4))(permuted_attn1_output)
    x = Dropout(dropout_rate)(x)

    # Block 2 Base (Lighter)
    F2 = 16; kernLength2 = 15
    x = SeparableConv2D(F2, (1, kernLength2), padding='same', use_bias=False, depthwise_regularizer=l2(l2_rate), pointwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x_act2 = Activation('elu')(x) # Output shape: (batch, Chans, Samples/4, 16)

    # Attention 2
    shape_b2 = tf.keras.backend.int_shape(x_act2)
    permuted_b2 = Permute((2, 1, 3))(x_act2)
    features_dim2 = shape_b2[1] * shape_b2[3] # Chans * 16
    reshaped_attn2_input = Reshape((-1, features_dim2))(permuted_b2)
    key_dim2 = max(1, features_dim2 // num_heads)
    logging.debug(f"  Q MHA 2: Input={reshaped_attn2_input.shape}, Heads={num_heads}, KeyDim={key_dim2}")
    attn2_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim2)(query=reshaped_attn2_input, value=reshaped_attn2_input, key=reshaped_attn2_input)
    attn2_output = LayerNormalization()(attn2_output + reshaped_attn2_input)
    reshaped_attn2_output = Reshape((shape_b2[2], shape_b2[1], shape_b2[3]))(attn2_output)
    permuted_attn2_output = Permute((2, 1, 3))(reshaped_attn2_output)

    # Continue Block 2
    x = AveragePooling2D((1, 8))(permuted_attn2_output)
    x = Dropout(dropout_rate)(x)

    # Classification Head (Lighter)
    x = Flatten()(x)
    x = Dense(32, activation='elu', kernel_regularizer=l2(l2_rate))(x) # Reduced dense
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs, name='build_q_eegnet_mha')



import time
import numpy as np
from tensorflow.keras.constraints import max_norm
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv2D, DepthwiseConv2D, SeparableConv2D, BatchNormalization,
    Activation, Dropout, AveragePooling2D, Flatten, Dense, Reshape,
    Permute, LayerNormalization, MultiHeadAttention
)
from tensorflow.keras.regularizers import l2
import tensorflow as tf
import logging
# Assuming you've already built your model using the build_q_eegnet function
#model_name = 'q_eegnet'
model = build_q_eegnet( #build_q_eegnet_mha(  #build_eegnet_msd_mha(  #build_eegnet_mha( #build_eegnet(  #build_eegnet_msd( #   build_q_eegnet(
    input_shape=current_input_shape,
    num_classes=NUM_CLASSES,
    dropout_rate=dropout,
    l2_rate=l2r,
    max_norm_val=1.0,
    num_heads=num_heads
)

# Generate a sample input (replace this with an actual sample from your dataset)
# For example, if the input shape is (batch_size, channels, time_steps), generate a random tensor
#sample_input = np.random.random((1, *current_input_shape))  # Assuming batch size of 1
sample_input = X_tr_c[0:1]
# Measure the time taken for a single forward pass (inference)
start_time = time.time()

# Perform the forward pass
predictions = model.predict(sample_input, verbose=0)

# End time after prediction
end_time = time.time()

# Calculate the time taken for the forward pass in milliseconds
time_taken_ms = (end_time - start_time) * 1000

# Print the time taken
print(f"Time taken for a single forward pas s: {time_taken_ms:.4f} ms")

model.summary()





model.summary()

model.summary()

model.summary()

model.summary()

model.summary()

"""## time compute"""

model

times = []
for _ in range(100):
    start_time = time.time()

    # Perform the forward pass
    predictions = model.predict(sample_input, verbose=0)

    end_time = time.time()
    delay = end_time - start_time
    times.append(delay)

# Convert to NumPy array for easy stats computation
times = np.array(times)

mean_time = np.mean(times)
std_dev = np.std(times)
min_time = np.min(times)
max_time=np.max(times)
print(f"Prediction time over 10 runs:")
print(f"Mean: {mean_time*1000:.2f} ms")
print(f"Standard Deviation: {std_dev*1000:.2f} ms")
print(f"Range: {min_time*1000:.2f} - {max_time*1000:.2f} ms")

print(f"{mean_time*1000:.2f}\t{std_dev*1000:.2f}\t{min_time*1000:.2f}\t{max_time*1000:.2f}")

times = []
for _ in range(100):
    start_time = time.time()

    # Perform the forward pass
    predictions = model.predict(sample_input, verbose=0)

    end_time = time.time()
    delay = end_time - start_time
    times.append(delay)

# Convert to NumPy array for easy stats computation
times = np.array(times)

mean_time = np.mean(times)
std_dev = np.std(times)
min_time = np.min(times)
max_time=np.max(times)
print(f"Prediction time over 10 runs:")
print(f"Mean: {mean_time*1000:.2f} ms")
print(f"Standard Deviation: {std_dev*1000:.2f} ms")
print(f"Range: {minn*1000:.2f} - {maxx*1000:.2f} ms")

print(f"{mean_time*1000:.2f}\t{std_dev*1000:.2f}\t{min_time*1000:.2f}\t{max_time*1000:.2f}")

start_time = time.time()

# Perform the forward pass
predictions = model.predict(sample_input, verbose=0)

# End time after prediction
end_time = time.time()
normal_delay = start_time- end_time
normal_delay

start_time = time.time()

# Perform the forward pass
predictions = model.predict(sample_input, verbose=0)

# End time after prediction
end_time = time.time()
normal_delay = start_time- end_time
normal_delay

start_time = time.time()

# Perform the forward pass
predictions = model.predict(sample_input, verbose=0)

# End time after prediction
end_time = time.time()
normal_delay = start_time- end_time
normal_delay



fit_start_time = time.time()

history = model.fit(
    X_tr_c, y_tr_c,
    epochs= EPOCHS_GRID,
    batch_size=batch,
    verbose=0,
    validation_data=(X_val_c, y_val_c),
    callbacks=callbacks,
    shuffle=True
)

'''
history = model.fit(X_tr_c, y_tr_c,
                            epochs=  EPOCHS_GRID,
                            batch_size=batch,
                            verbose=0, # Set to 1 or 2 for more detailed Keras output per epoch
                            validation_data=(X_val_c, y_val_c),
                            callbacks=callbacks, # Includes EpochLogger, Checkpoints, EarlyStopping, Timing
                            shuffle=True)

'''
        #training_time = timing_callback.get_total_time()

        # Classification Report and Confusion Matrix
y_pred_probs = model.predict(X_val_c, batch_size=batch)
y_pred_labels = np.argmax(y_pred_probs, axis=1) # Predicted class indices (0 or 1)
y_true_labels = np.argmax(y_val_c, axis=1)     # True class indices (0 or 1)

        #training_time = timing_callback.get_total_time() if 'timing_callback' in locals() else -1.0
epochs_ran = len(history.history['loss']) if history and 'loss' in history.history else 0
        #logging.error(f"An error occurred during training or evaluation: {fit_eval_error}", exc_info=True)
val_acc, val_loss = -1.0, -1.0; precision, recall, f1_score = -1.0, -1.0, -1.0
        #current_error = f"FitEvalError: {fit_eval_error}"

    # --- Plotting ---
if history and epochs_ran > 0: # Only plot if training actually happened
        logging.info("Generating training history plots...")
        #epoch_times_list = timing_callback.get_epoch_times()[:epochs_ran]
        # Pad with 0 for the start time at epoch 0
        #cumulative_time = np.cumsum([0] + epoch_times_list)
        epoch_axis = range(epochs_ran) # Epochs are 0 to epochs_ran-1

        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
#        fig.suptitle(f'Run {i+1}: {model_name}\n{config_str}', fontsize=9) # Slightly smaller font
        fig.suptitle(f'Run {0+1}: {model_name}\n', fontsize=9) # Slightly smaller font

        # Accuracy Plot
if 'accuracy' in history.history and 'val_accuracy' in history.history:
            axes[0].plot(epoch_axis, history.history['accuracy'], label='Train Acc')
            axes[0].plot(epoch_axis, history.history['val_accuracy'], label='Val Acc')
            axes[0].set_title('Accuracy vs. Epoch'); axes[0].set_xlabel('Epoch'); axes[0].legend(); axes[0].grid(True)
else: axes[0].set_title('Accuracy Plot Unavailable'); axes[0].text(0.5, 0.5, 'No Data', ha='center', va='center')

        # Loss Plot
if 'loss' in history.history and 'val_loss' in history.history:
            axes[1].plot(epoch_axis, history.history['loss'], label='Train Loss')
            axes[1].plot(epoch_axis, history.history['val_loss'], label='Val Loss')
            axes[1].set_title('Loss vs. Epoch'); axes[1].set_xlabel('Epoch'); axes[1].legend(); axes[1].grid(True)
else: axes[1].set_title('Loss Plot Unavailable'); axes[1].text(0.5, 0.5, 'No Data', ha='center', va='center')

        # Cumulative Time Plot
        # Need epochs_ran + 1 points for cumulative time plot (includes time=0 at epoch 0)
#---

plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap

plt.show()

plt.close(fig)



"""## EXP - 1 ) trying two class - with small 500 ms window"""

def prepare_data_from_gdf_aad_500ms(raw, events, sfreq, internal_label_map, include_no_movement):
    """Creates 500 ms epochs for left, right, and optionally no movement classes, with artifact rejection."""
    if raw is None or events is None or sfreq <= 0:
        return None, None, 0

    #<><><><>< start: initialize final dataset lists
    X_list = []
    y_list = []
    #<><><><>< end

    #<><><><>< start: define epoch parameters
    movement_start = 2.0  # seconds after event
    movement_end = 6.0
    no_movement_start = 0.0
    no_movement_end = 2.0
    epoch_len = 0.5  # seconds
    samples_per_epoch = int(sfreq * epoch_len)
    #<><><><>< end

    try:
        for ev in events:
            ev_time = ev[0]  # sample index
            ev_id = ev[2]

            # Convert event time from samples to seconds
            ev_time_sec = ev_time / sfreq

            #<><><><>< start: left and right class epochs (movement)
            if ev_id in [7, 8]:  # 7: Left, 8: Right
                start = ev_time_sec + movement_start
                end = ev_time_sec + movement_end
                num_epochs = int((end - start) / epoch_len)

                for i in range(num_epochs):
                    epoch_start_sec = start + i * epoch_len
                    epoch_start_sample = int(epoch_start_sec * sfreq)
                    epoch_end_sample = epoch_start_sample + samples_per_epoch
                    if epoch_end_sample > raw.n_times:
                        break
                    epoch_data, _ = raw[:, epoch_start_sample:epoch_end_sample]
                    if epoch_data.shape[1] == samples_per_epoch:
                        X_list.append(epoch_data)
                        y_list.append(internal_label_map[ev_id])
            #<><><><>< end

            #<><><><>< start: optional "no movement" epochs
            if include_no_movement:
                start = ev_time_sec + no_movement_start
                end = ev_time_sec + no_movement_end
                num_epochs = int((end - start) / epoch_len)

                for i in range(num_epochs):
                    epoch_start_sec = start + i * epoch_len
                    epoch_start_sample = int(epoch_start_sec * sfreq)
                    epoch_end_sample = epoch_start_sample + samples_per_epoch
                    if epoch_end_sample > raw.n_times:
                        break
                    epoch_data, _ = raw[:, epoch_start_sample:epoch_end_sample]
                    if epoch_data.shape[1] == samples_per_epoch:
                        X_list.append(epoch_data)
                        y_list.append(2)  # Label for no movement
            #<><><><>< end

    except Exception as e:
        logging.error(f"Epoch slicing error: {e}")
        return None, None, 0

    #<><><><>< start: convert list to numpy arrays and apply artifact rejection
    X = np.array(X_list)  # shape: (n_epochs, n_channels, n_samples)
    y = np.array(y_list)

    # Reject epochs with peak-to-peak > 150 µV
    reject_threshold = 150e-6  # 150 µV
    good_indices = []
    for idx, x in enumerate(X):
        ptp_amplitudes = np.ptp(x, axis=1)  # peak-to-peak per channel
        if np.any(ptp_amplitudes > reject_threshold):
            continue
        good_indices.append(idx)

    X = X[good_indices]
    y = y[good_indices]
    #<><><><>< end

    #<><><><>< start: expand X for Conv2D
    X = X[..., np.newaxis]  # shape: (n_epochs, n_channels, n_samples, 1)
    #<><><><>< end

    #<><><><>< start: display stats
    logging.info(f"Final Epoch shape: {X.shape}")
    logging.info(f"Final Labels shape: {y.shape}")
    unique, counts = np.unique(y, return_counts=True)
    class_distribution = dict(zip(unique, counts))
    logging.info(f"Class distribution (label: count): {class_distribution}")
    #<><><><>< end

    return X, y, samples_per_epoch

print("hi")

# --- Data Preparation ---
# Pass the original EVENT_ID_MAPPING { '769': 7, '770': 8 }
raw_obj, events_array, sfreq, event_map_for_epochs = load_and_preprocess_gdf(
    GDF_FILE_PATH, low_f, high_f, EVENT_ID_MAPPING
)

#-------------------------------------->>> EPOCH DATA STRUCTURE  change her onwards if diff epoching str START_------------------------

# Pass event_map_for_epochs ({ '769': 7, '770': 8 }) and internal_label_map ({7:0, 8:1})
X_data, y_data, actual_samples = prepare_data_from_gdf_aad_500ms(
#    raw_obj, events_array, sfreq, t_min, t_max, event_map_for_epochs, INTERNAL_LABEL_MAP
#    raw_obj, events_array, sfreq, 0, 3, event_map_for_epochs, INTERNAL_LABEL_MAP
    raw_obj, events_array, sfreq, INTERNAL_LABEL_MAP ,  include_no_movement=0
)





del raw_obj, events_array # Free memory
if X_data is None or actual_samples == 0:
    print("Epoch extraction or artifact rejection failed. Skipping combination.")


# Convert labels to categorical format for TF/Keras
y_data_cat = tf.keras.utils.to_categorical(y_data, num_classes=NUM_CLASSES)

# --- Split data ---

X_tr_c, X_val_c, y_tr_c, y_val_c = train_test_split(
        X_data, y_data_cat, test_size=0.2, random_state=42, stratify=y_data # Stratify by original integer labels
    )



#------------------> to save model with val acc

import os
import tensorflow as tf
from tensorflow.keras.callbacks import Callback

class SaveBestModelWithValAcc(Callback):
    def __init__(self, save_dir='models', prefix='best_model'):
        super().__init__()
        self.best_val_acc = -1
        self.save_dir = save_dir
        self.prefix = prefix
        os.makedirs(save_dir, exist_ok=True)

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        val_acc = logs.get('val_accuracy')
        if val_acc is not None and val_acc > self.best_val_acc:
            self.best_val_acc = val_acc
            filename = f"{self.prefix}_valacc_{val_acc:.4f}.keras"
            filepath = os.path.join(self.save_dir, filename)
            self.model.save(filepath)
            print(f"✅ Saved new best model to {filepath}")

callbacks = [
    EpochLogger(),
    SaveBestModelWithValAcc(save_dir='saved_models', prefix='EEGNet')
]



if opt.lower() == 'adam': optimizer = Adam(learning_rate=lr_to_use)
optimizer = Adam(learning_rate=lr_to_use)
current_input_shape = X_tr_c.shape[1:] # Shape is (Channels, Samples, 1)
        # Call the function stored in model_builder (e.g., build_eegnet_msd)
        # Pass necessary parameters, kwargs catch extras like num_heads for models that don't use them

#----
model_name = 'q_eegnet'
model = build_q_eegnet( #model_builder(
            input_shape=current_input_shape,
            num_classes=NUM_CLASSES,
            dropout_rate=dropout,
            l2_rate=l2r,
            max_norm_val=max_norm, # Pass max_norm (used by MSD models)
            num_heads=num_heads    # Pass num_heads (used by MHA models)
        )

model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
print("h")

fit_start_time = time.time()

history = model.fit(
    X_tr_c, y_tr_c,
    epochs= EPOCHS_GRID,
    batch_size=batch,
    verbose=0,
    validation_data=(X_val_c, y_val_c),
    callbacks=callbacks,
    shuffle=True
)

'''
history = model.fit(X_tr_c, y_tr_c,
                            epochs=  EPOCHS_GRID,
                            batch_size=batch,
                            verbose=0, # Set to 1 or 2 for more detailed Keras output per epoch
                            validation_data=(X_val_c, y_val_c),
                            callbacks=callbacks, # Includes EpochLogger, Checkpoints, EarlyStopping, Timing
                            shuffle=True)

'''
        #training_time = timing_callback.get_total_time()

        # Classification Report and Confusion Matrix
y_pred_probs = model.predict(X_val_c, batch_size=batch)
y_pred_labels = np.argmax(y_pred_probs, axis=1) # Predicted class indices (0 or 1)
y_true_labels = np.argmax(y_val_c, axis=1)     # True class indices (0 or 1)

        #training_time = timing_callback.get_total_time() if 'timing_callback' in locals() else -1.0
epochs_ran = len(history.history['loss']) if history and 'loss' in history.history else 0
        #logging.error(f"An error occurred during training or evaluation: {fit_eval_error}", exc_info=True)
val_acc, val_loss = -1.0, -1.0; precision, recall, f1_score = -1.0, -1.0, -1.0
        #current_error = f"FitEvalError: {fit_eval_error}"

    # --- Plotting ---
if history and epochs_ran > 0: # Only plot if training actually happened
        logging.info("Generating training history plots...")
        #epoch_times_list = timing_callback.get_epoch_times()[:epochs_ran]
        # Pad with 0 for the start time at epoch 0
        #cumulative_time = np.cumsum([0] + epoch_times_list)
        epoch_axis = range(epochs_ran) # Epochs are 0 to epochs_ran-1

        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
#        fig.suptitle(f'Run {i+1}: {model_name}\n{config_str}', fontsize=9) # Slightly smaller font
        fig.suptitle(f'Run {0+1}: {model_name}\n', fontsize=9) # Slightly smaller font

        # Accuracy Plot
if 'accuracy' in history.history and 'val_accuracy' in history.history:
            axes[0].plot(epoch_axis, history.history['accuracy'], label='Train Acc')
            axes[0].plot(epoch_axis, history.history['val_accuracy'], label='Val Acc')
            axes[0].set_title('Accuracy vs. Epoch'); axes[0].set_xlabel('Epoch'); axes[0].legend(); axes[0].grid(True)
else: axes[0].set_title('Accuracy Plot Unavailable'); axes[0].text(0.5, 0.5, 'No Data', ha='center', va='center')

        # Loss Plot
if 'loss' in history.history and 'val_loss' in history.history:
            axes[1].plot(epoch_axis, history.history['loss'], label='Train Loss')
            axes[1].plot(epoch_axis, history.history['val_loss'], label='Val Loss')
            axes[1].set_title('Loss vs. Epoch'); axes[1].set_xlabel('Epoch'); axes[1].legend(); axes[1].grid(True)
else: axes[1].set_title('Loss Plot Unavailable'); axes[1].text(0.5, 0.5, 'No Data', ha='center', va='center')

        # Cumulative Time Plot
        # Need epochs_ran + 1 points for cumulative time plot (includes time=0 at epoch 0)
#---

plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap

plt.show()

plt.close(fig)

