# -*- coding: utf-8 -*-
"""transf-learn-using-utk-code- PART 0- BCI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xC62EAFt1_XxPa9ORdEWbjcmYIFGV04t
"""

import mne

raw = mne.io.read_raw_eeglab("/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_Right_10trials.set", preload=False)
print("EEG Channels in this file:")
print(raw.ch_names)


bci42a_channels = [
    'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4',
    'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6',
    'CP3', 'CP1', 'CPz', 'CP2', 'CP4',
    'P1', 'Pz', 'P2', 'POz'
]

aad_custom_ch= ['C3', 'C4', 'Cz']

"""## step 0 ) functions"""

import numpy as np
import pandas as pd
import mne

def extract_epochs_from_set_and_csv(set_file, csv_file, ch_name_list, label, bandpass=(5., 35.)):
    """
    Extracts EEG segments from a .set file based on start-stop times in a CSV file.

    Args:
        set_file (str): Path to the .set EEG file.
        csv_file (str): CSV file with 'start' and 'stop' columns (in seconds).
        label (int): Integer label to assign to this class (e.g., 0 for left, 1 for right).
        bandpass (tuple): (low_freq, high_freq) bandpass filter in Hz.

    Returns:
        X (np.ndarray): EEG segments of shape (n_trials, channels, samples, 1)
        y (np.ndarray): Labels of shape (n_trials,)
        sfreq (float): Sampling frequency
    """
    segment_duration= 3.0 #--- 3 sec fixed duration otherwise error that alll train samples of varying len
    raw = mne.io.read_raw_eeglab(set_file, preload=True)
#----------> choose channnels
    raw.pick_channels(ch_name_list)  # or use the full BCI channel list

    raw.filter(bandpass[0], bandpass[1], fir_design='firwin')
    sfreq = raw.info['sfreq']
    data = raw.get_data()  # shape: (channels, samples)

    df = pd.read_csv(csv_file)
    X_list = []
    for _, row in df.iterrows():
        start_sec = float(row['start'])
        stop_sec = float(row['stop'])
        start_sample = int(start_sec * sfreq)
        #stop_sample = int(stop_sec * sfreq)
        stop_sample = start_sample + int(segment_duration * sfreq)


        if stop_sample > data.shape[1]:
            print(f"Warning: stop sample {stop_sample} exceeds EEG length. Skipping this segment.")
            continue

        segment = data[:, start_sample:stop_sample]  # shape: (channels, samples_in_segment)

        # Skip very short segments
        if segment.shape[1] < int(0.5 * sfreq):  # Less than 0.5 sec
            print(f"Warning: segment too short ({segment.shape[1]} samples). Skipping.")
            continue

        segment = segment[..., np.newaxis]  # Add channel dimension → (channels, samples, 1)
        X_list.append(segment)

    X = np.array(X_list)
    y = np.full((len(X_list),), label)
    return X, y, sfreq



def combine_and_shuffle_epochs (X_list, y_list, seed=42):  #(X1, y1, X2, y2, seed=42):
    """
    Combines and shuffles two sets of EEG epochs with their labels.

    Args:
        X1, y1: Data and labels from class 1
        X2, y2: Data and labels from class 2

    Returns:
        X_combined: Shuffled EEG data
        y_combined: Corresponding shuffled labels
    """
    X = np.concatenate( X_list, axis=0)       #[X1, X2], axis=0)
    y = np.concatenate( y_list, axis=0)                  #[y1, y2], axis=0)

    # Shuffle
    idx = np.random.RandomState(seed=seed).permutation(len(y))
    X_shuffled = X[idx]
    y_shuffled = y[idx]

    return X_shuffled, y_shuffled



'''#----------------- LEFT ------------------
label_L = 1
csv_f= "/kaggle/input/timestamps/label and timestamps csv/aadhar_bicepcurl_Left_10trials.csv"
set_f= "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_Left_10trials.set"
freq_band_filt = (8., 30.)
X_L, y_L, sfreq_L = extract_epochs_from_set_and_csv(set_f, csv_f, label_L, bandpass=freq_band_filt)

#----------------- RIGHT ------------------
label_r = 2
csv_f= "/kaggle/input/timestamps/label and timestamps csv//kaggle/input/timestamps/label and timestamps csv/aadhar_bicepcurl_Right_10trials.csv"
set_f= "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files//kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_Right_10trials.set"
X_r, y_r, sfreq_R = extract_epochs_from_set_and_csv(set_f, csv_f, label_L, bandpass=freq_band_filt)'''

print(X_L.shape)
y_L.shape

"""## data prep"""

import os, time, numpy as np, tensorflow as tf
from sklearn.model_selection import train_test_split


# --- Config ---
set_l_1 = "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_Left_10trials.set"
set_r_1= "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_Right_10trials.set"
csv_left_1 = "/kaggle/input/timestamps/label and timestamps csv/aadhar_bicepcurl_Left_10trials.csv"
csv_right_1 = "//kaggle/input/timestamps/label and timestamps csv/aadhar_bicepcurl_Right_10trials.csv"

set_l_2 = "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_LeftRRight_15trials.set"
set_r_2 = set_l_2
csv_left_2 = "/kaggle/input/timestamps/label and timestamps csv/aadhar_bicepcurl_LeftRRight_15trials____LEFT_ONLY___.csv"
csv_right_2 = "/kaggle/input/timestamps/label and timestamps csv/aadhar_bicepcurl_LeftRRight_15trials____RIGHT_ONLY___.csv"


label_left = 0
label_right = 1
NUM_CLASSES = 2
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# --- Prepare Data ---

#Left
X_left_1, y_left_1, sfreq = extract_epochs_from_set_and_csv(set_l_1, csv_left_1, label=label_left)
X_left_2, y_left_2, _ = extract_epochs_from_set_and_csv(set_l_2, csv_left_2, label=label_left)

X_right_1, y_right_1, _ = extract_epochs_from_set_and_csv(set_r_1, csv_right_1, label=label_right)
X_right_2, y_right_2, _ = extract_epochs_from_set_and_csv(set_r_2, csv_right_2, label=label_right)


#X_all, y_all = combine_and_shuffle_epochs(X_left, y_left, X_right, y_right)
X_all, y_all = combine_and_shuffle_epochs ([X_left_1,X_right_1,X_left_2,X_right_2], [y_left_1,y_right_1,y_left_2,y_right_2])


                            y_cat = to_categorical(y_all, num_classes=NUM_CLASSES)
X_train, X_val, y_train, y_val = train_test_split(X_all, y_cat, test_size=0.3, stratify=y_all, random_state=42)

y_val

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv2D, DepthwiseConv2D, SeparableConv2D,
    BatchNormalization, Activation, AveragePooling2D,
    Dropout, Flatten, Dense
)
from tensorflow.keras.regularizers import l2

# --- Model ---

def build_q_eegnet(input_shape, num_classes=2, dropout_rate=0.5, l2_rate=1e-4, **kwargs):
    """Placeholder 'Quantized-like' EEGNet (Lighter variant for demonstration)."""
    n_channels, n_times, _ = input_shape
    inputs = Input(shape=input_shape, name='Input')
    # Block 1 - Reduced Filters/Depth Multiplier
    F1 = 8 # Reduced from 16 in build_eegnet
    D = 1 # Reduced from 2
    kernLength = 51 # Keep same as build_eegnet
    x = Conv2D(F1, (1, kernLength), padding='same', use_bias=False, kernel_regularizer=l2(l2_rate))(inputs)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((n_channels, 1), depth_multiplier=D, use_bias=False, depthwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 4))(x)
    x = Dropout(dropout_rate)(x)
    # Block 2 - Reduced Filters
    F2 = 16 # Reduced from 32
    kernLength2 = 15 # Keep same as build_eegnet
    x = SeparableConv2D(F2, (1, kernLength2), padding='same', use_bias=False, depthwise_regularizer=l2(l2_rate), pointwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 8))(x)
    x = Dropout(dropout_rate)(x)
    # Classification Head - Reduced Dense Layer
    x = Flatten()(x)
    x = Dense(32, activation='elu', kernel_regularizer=l2(l2_rate))(x) # Reduced from 64
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs, name='build_q_eegnet')


input_shape = X_train.shape[1:]
model = build_q_eegnet(input_shape=input_shape, num_classes=NUM_CLASSES, dropout_rate=0.25, l2_rate=0.0001)
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# --- Logging Callback ---
class EpochLogger(Callback):
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        print(f"Epoch {epoch+1}: Train Acc={logs.get('accuracy', -1):.4f}, Val Acc={logs.get('val_accuracy', -1):.4f}")

# --- Save Best Model Callback ---
class SaveBestModel(Callback):
    def __init__(self, path='best_model.keras'):
        super().__init__()
        self.best_val = -1
        self.path = path

    def on_epoch_end(self, epoch, logs=None):
        acc = logs.get('val_accuracy', 0)
        if acc > self.best_val:
            self.best_val = acc
            self.model.save(self.path)
            print(f"✔️ New best model saved with val_acc={acc:.4f}")

callbacks = [EpochLogger(), SaveBestModel()]

# --- Train ---
model.fit(X_train, y_train,
          validation_data=(X_val, y_val),
          batch_size=64,
          epochs= 300,
          callbacks=callbacks,
          verbose=0,
          shuffle=True)

# --- Evaluate ---
val_preds = np.argmax(model.predict(X_val), axis=1)
val_true = np.argmax(y_val, axis=1)
acc = np.mean(val_preds == val_true)
print(f"Final Validation Accuracy: {acc:.4f}")

y_pred_probs = model.predict(X_val, batch_size=64)
y_pred_labels = np.argmax(y_pred_probs, axis=1) # Predicted class indices (0 or 1)
y_true_labels = np.argmax(y_val, axis=1)     # True class indices (0 or 1)

print(y_pred_labels)
print(y_true_labels)

y_pred_probs = model.predict(X_val, batch_size=64)
y_pred_labels = np.argmax(y_pred_probs, axis=1) # Predicted class indices (0 or 1)
y_true_labels = np.argmax(y_val, axis=1)     # True class indices (0 or 1)

print(y_pred_labels)
print(y_true_labels)

"""## step 1) data"""

import os, time, numpy as np, tensorflow as tf
from sklearn.model_selection import train_test_split

bandpass=(8., 20.)


bci42a_channels = [
    'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4',
    'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6',
    'CP3', 'CP1', 'CPz', 'CP2', 'CP4',
    'P1', 'Pz', 'P2', 'POz'
]

aad_custom_ch= ['C3', 'C4', 'Cz',]


# --- Config ---
set_l_1 = "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_Left_10trials.set"
set_r_1= "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_Right_10trials.set"
csv_left_1 = "/kaggle/input/timest/label and timestamps csv/aadhar_bicepcurl_Left_10trials.csv"
csv_right_1 = "//kaggle/input/timest/label and timestamps csv/aadhar_bicepcurl_Right_10trials.csv"

set_l_2 = "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_LeftRRight_15trials.set"
set_r_2 = set_l_2
csv_left_2 = "/kaggle/input/timest/label and timestamps csv/aadhar_bicepcurl_LeftRRight_15trials____LEFT_ONLY___.csv"
csv_right_2 = "/kaggle/input/timest/label and timestamps csv/aadhar_bicepcurl_LeftRRight_15trials____RIGHT_ONLY___.csv"


label_left = 0
label_right = 1
NUM_CLASSES = 2
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# --- Prepare Data ---


chann_list= aad_custom_ch

#Left
X_left_1, y_left_1, sfreq = extract_epochs_from_set_and_csv(set_l_1, csv_left_1, chann_list, label=label_left, bandpass=(8., 20.))
X_left_2, y_left_2, _ = extract_epochs_from_set_and_csv(set_l_2, csv_left_2, chann_list, label=label_left, bandpass=(8., 20.))

X_right_1, y_right_1, _ = extract_epochs_from_set_and_csv(set_r_1, csv_right_1, chann_list, label=label_right,  bandpass=(8., 20.))
X_right_2, y_right_2, _ = extract_epochs_from_set_and_csv(set_r_2, csv_right_2, chann_list, label=label_right, bandpass=(8., 20.))


#X_all, y_all = combine_and_shuffle_epochs(X_left, y_left, X_right, y_right)
X_all, y_all = combine_and_shuffle_epochs ([X_left_1,X_right_1,X_left_2,X_right_2], [y_left_1,y_right_1,y_left_2,y_right_2])


y_cat = to_categorical(y_all, num_classes=NUM_CLASSES)
X_train, X_val, y_train, y_val = train_test_split(X_all, y_cat, test_size=0.3, stratify=y_all, random_state=42)

"""## step 2) train"""

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv2D, DepthwiseConv2D, SeparableConv2D,
    BatchNormalization, Activation, AveragePooling2D,
    Dropout, Flatten, Dense
)
from tensorflow.keras.regularizers import l2

# --- Model ---

def build_q_eegnet(input_shape, num_classes=2, dropout_rate=0.5, l2_rate=1e-4, **kwargs):
    """Placeholder 'Quantized-like' EEGNet (Lighter variant for demonstration)."""
    n_channels, n_times, _ = input_shape
    inputs = Input(shape=input_shape, name='Input')
    # Block 1 - Reduced Filters/Depth Multiplier
    F1 = 8 # Reduced from 16 in build_eegnet
    D = 1 # Reduced from 2
    kernLength = 51 # Keep same as build_eegnet
    x = Conv2D(F1, (1, kernLength), padding='same', use_bias=False, kernel_regularizer=l2(l2_rate))(inputs)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((n_channels, 1), depth_multiplier=D, use_bias=False, depthwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 4))(x)
    x = Dropout(dropout_rate)(x)
    # Block 2 - Reduced Filters
    F2 = 16 # Reduced from 32
    kernLength2 = 15 # Keep same as build_eegnet
    x = SeparableConv2D(F2, (1, kernLength2), padding='same', use_bias=False, depthwise_regularizer=l2(l2_rate), pointwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 8))(x)
    x = Dropout(dropout_rate)(x)
    # Classification Head - Reduced Dense Layer
    x = Flatten()(x)
    x = Dense(32, activation='elu', kernel_regularizer=l2(l2_rate))(x) # Reduced from 64
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs, name='build_q_eegnet')


input_shape = X_train.shape[1:]
model = build_q_eegnet(input_shape=input_shape, num_classes=NUM_CLASSES, dropout_rate=0.25, l2_rate=0.0001)
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# --- Logging Callback ---
class EpochLogger(Callback):
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        print(f"Epoch {epoch+1}: Train Acc={logs.get('accuracy', -1):.4f}, Val Acc={logs.get('val_accuracy', -1):.4f}")

# --- Save Best Model Callback ---
class SaveBestModel(Callback):
    def __init__(self, path='best_model.keras'):
        super().__init__()
        self.best_val = -1
        self.path = path

    def on_epoch_end(self, epoch, logs=None):
        acc = logs.get('val_accuracy', 0)
        if acc > self.best_val:
            self.best_val = acc
            self.model.save(self.path)
            print(f"✔️ New best model saved with val_acc={acc:.4f}")

callbacks = [EpochLogger(), SaveBestModel()]

# --- Train ---
model.fit(X_train, y_train,
          validation_data=(X_val, y_val),
          batch_size=64,
          epochs= 300,
          callbacks=callbacks,
          verbose=0,
          shuffle=True)

# --- Evaluate ---
val_preds = np.argmax(model.predict(X_val), axis=1)
val_true = np.argmax(y_val, axis=1)
acc = np.mean(val_preds == val_true)
print(f"Final Validation Accuracy: {acc:.4f}")

y_train.shape

"""## -----------PART 2 A- full train"""

#---------------- compatibility ensure:

#--- downsample
    # after creating the data from aad eeg, et the end downsample and set new samling freq





"""## ------- PART 0) BCI train"""

# --- Data Preprocessing Function (Using GDF Loader Workaround) ---
def load_and_preprocess_gdf(gdf_file_path, low_freq, high_freq, event_id_mapping):
    """Loads GDF, applies filtering based on parameters, using a two-step load."""
    if not os.path.exists(gdf_file_path):
        raise FileNotFoundError(f"GDF file not found: {gdf_file_path}")
    raw = None
    try:
        logging.info(f"Loading GDF header/annotations: {gdf_file_path}...")
        # Load annotations first, but not data yet
        raw = mne.io.read_raw_gdf(gdf_file_path, preload=False, verbose='warning')
        logging.info("Header and annotations loaded.")
        sfreq = raw.info['sfreq']

        logging.info(f"Extracting events from annotations using mapping: {event_id_mapping}...")
        # Use the user-provided mapping { '769': 7, '770': 8 }
        events, event_dict_mne_internal = mne.events_from_annotations(raw, event_id=event_id_mapping, verbose='warning')
        # event_dict_mne_internal will likely be {'Left Hand (769)': 7, 'Right Hand (770)': 8}
        logging.info(f"Found {len(events)} events corresponding to specified annotations.")
        if len(events) == 0:
            raise ValueError("No relevant events found via annotations using the provided mapping.")

        logging.info("Loading GDF data into memory...")
        raw.load_data(verbose='warning')
        logging.info("Data loaded into memory.")

        # Pick only EEG channels (first 22 as per PDF description)
        try:
            # Explicitly pick the first 22 channels if they represent EEG
            num_eeg_channels = 22
            if len(raw.ch_names) >= num_eeg_channels:
                 picks = raw.ch_names[:num_eeg_channels]
                 raw.pick(picks=picks)
                 logging.info(f"Selected the first {num_eeg_channels} channels assumed to be EEG: {raw.ch_names}")
            else:
                 logging.warning(f"Found fewer than {num_eeg_channels} channels. Using all {len(raw.ch_names)} channels.")
                 raw.pick(picks='eeg', errors='ignore') # Fallback to picking by type 'eeg' if names aren't reliable

            # Exclude EOG (usually last 3 channels in this dataset)
            raw.drop_channels([ch for ch in raw.ch_names if 'EOG' in ch.upper()], on_missing='ignore')
            logging.info(f"Channels after picking EEG and dropping EOG (if found): {len(raw.ch_names)}")

        except Exception as e:
            logging.warning(f"Error picking/dropping channels: {e}. Proceeding with available channels.")

        logging.info(f"Applying band-pass filter ({low_freq:.1f}-{high_freq:.1f} Hz)...")
        raw.filter(low_freq, high_freq, fir_design='firwin', skip_by_annotation='edge', verbose='warning')

        # Pass the original event_id_mapping { '769': 7, '770': 8 } for epoching
        return raw, events, sfreq, event_id_mapping

    except ValueError as e:
        logging.error(f"Event extraction error: {e}.")
        if raw:
             logging.error(f"Available annotation descriptions: {np.unique(raw.annotations.description)}")
        return None, None, 0, None
    except Exception as e:
        logging.error(f"Error during GDF loading/preprocessing: {e}", exc_info=True)
        return None, None, 0, None

print("cell -2")

# --- Data Preparation Function ---
def prepare_data_from_gdf(raw, events, sfreq, tmin, tmax, event_id_map_for_epochs, internal_label_map):
    """Creates epochs, applies artifact rejection, and prepares data arrays."""
    if raw is None or events is None or sfreq <= 0: return None, None, 0

    duration = tmax - tmin
    expected_samples = int(math.ceil(duration * sfreq))
    logging.info(f"Creating epochs ({tmin:.1f}s to {tmax:.1f}s relative to events -> {expected_samples} samples)...")
    try:
        # Use the { '769': 7, '770': 8 } mapping here for creating epochs
        epochs = mne.Epochs(raw, events, event_id=event_id_map_for_epochs, tmin=tmin, tmax=tmax,
                           baseline=None, # No baseline correction in this window
                           preload=True, verbose='warning')
        logging.info(f"Initial epochs created: {len(epochs)}")
        if len(epochs) == 0: raise ValueError("Epoch object empty after creation.")

        # --- ADDED: Basic Artifact Rejection ---
        original_num_epochs = len(epochs)
        # Simple peak-to-peak rejection - adjust threshold as needed
        reject_criteria = dict(eeg=150e-6) # Reject epochs where any EEG channel exceeds 150 uV peak-to-peak
        try:
            epochs.drop_bad(reject=reject_criteria, verbose='warning')
            logging.info(f"Epochs after PTP rejection (> {reject_criteria['eeg']*1e6:.0f} uV): {len(epochs)} (dropped {original_num_epochs - len(epochs)})")
        except Exception as reject_err:
            logging.warning(f"Could not apply artifact rejection: {reject_err}. Using all created epochs.")

        if len(epochs) == 0:
            logging.error("No epochs remaining after artifact rejection.")
            return None, None, 0
        # --- End Artifact Rejection ---

    except Exception as e:
        logging.error(f"Epoch creation or rejection error: {e}")
        return None, None, 0

    actual_samples_per_epoch = epochs.get_data().shape[-1]
    if actual_samples_per_epoch != expected_samples:
        logging.warning(f"Actual samples ({actual_samples_per_epoch}) differs from expected ({expected_samples}). Using actual.")

    X = epochs.get_data(units='uV') # Get data in microvolts
    logging.info(f"Epoch data shape: {X.shape}")

    # Map the original MNE event IDs (7, 8) to internal labels (0, 1)
    y_integers = epochs.events[:, -1] # Get the event IDs (e.g., 7, 8)
    try:
        y = np.array([internal_label_map[event_id] for event_id in y_integers])
    except KeyError as e:
        logging.error(f"Label mapping error for event ID {e}. Check INTERNAL_LABEL_MAP: {internal_label_map}")
        return None, None, 0

    logging.info(f"Labels shape: {y.shape}, Unique internal labels: {np.unique(y)}, Distribution: {np.bincount(y)}")

    # Reshape for Conv2D input (add channel dimension)
    X = X[..., np.newaxis]
    logging.info(f"Reshaped X shape for model: {X.shape}")

    return X, y, actual_samples_per_epoch
print("cell 3")

import os
import numpy as np
import tensorflow as tf
import mne # For loading GDF and processing
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import itertools
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D, SeparableConv2D,
                                     BatchNormalization, Activation, AveragePooling2D,
                                     Dropout, Flatten, Dense, Concatenate)
from tensorflow.keras.constraints import max_norm # Re-added for EEGNet_MSD
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.optimizers.schedules import LearningRateSchedule
from tensorflow.keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint, EarlyStopping # Added EarlyStopping
import time
import logging
import math
import traceback

GDF_FILE_PATH= "/kaggle/input/bci-4-2a/A01T.gdf"

EVENT_ID_MAPPING = {'769': 7, '770': 8} # Using distinct integers > standard mne codes
INTERNAL_LABEL_MAP = {v: i for i, v in enumerate(sorted(EVENT_ID_MAPPING.values()))} # Maps 7->0, 8->1
NUM_CLASSES = len(EVENT_ID_MAPPING)
TARGET_NAMES = ['Left Hand (769)', 'Right Hand (770)']

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

raw_obj, events_array, sfreq, event_map_for_epochs = load_and_preprocess_gdf(
    GDF_FILE_PATH, 8.0, 30.0, EVENT_ID_MAPPING
)


# Pass event_map_for_epochs ({ '769': 7, '770': 8 }) and internal_label_map ({7:0, 8:1})
X_data, y_data, actual_samples = prepare_data_from_gdf(
#    raw_obj, events_array, sfreq, t_min, t_max, event_map_for_epochs, INTERNAL_LABEL_MAP
    raw_obj, events_array, sfreq, 0, 3, event_map_for_epochs, INTERNAL_LABEL_MAP
)
#del raw_obj, events_array # Free memory
if X_data is None or actual_samples == 0:
    print("Epoch extraction or artifact rejection failed. Skipping combination.")


# Convert labels to categorical format for TF/Keras
y_data_cat = tf.keras.utils.to_categorical(y_data, num_classes=NUM_CLASSES)

# --- Split data ---

X_tr_c, X_val_c, y_tr_c, y_val_c = train_test_split(
        X_data, y_data_cat, test_size=0.2, random_state=42, stratify=y_data # Stratify by original integer labels
    )

"""(115, 22, 751, 1) - trials, chann,"""

X_tr_c.shape

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv2D, DepthwiseConv2D, SeparableConv2D,
    BatchNormalization, Activation, AveragePooling2D,
    Dropout, Flatten, Dense
)
from tensorflow.keras.regularizers import l2

# --- Model ---

def build_q_eegnet(input_shape, num_classes=2, dropout_rate=0.5, l2_rate=1e-4, **kwargs):
    """Placeholder 'Quantized-like' EEGNet (Lighter variant for demonstration)."""
    n_channels, n_times, _ = input_shape
    inputs = Input(shape=input_shape, name='Input')
    # Block 1 - Reduced Filters/Depth Multiplier
    F1 = 8 # Reduced from 16 in build_eegnet
    D = 1 # Reduced from 2
    kernLength = 51 # Keep same as build_eegnet
    x = Conv2D(F1, (1, kernLength), padding='same', use_bias=False, kernel_regularizer=l2(l2_rate))(inputs)
    x = BatchNormalization()(x)
    x = DepthwiseConv2D((n_channels, 1), depth_multiplier=D, use_bias=False, depthwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 4))(x)
    x = Dropout(dropout_rate)(x)
    # Block 2 - Reduced Filters
    F2 = 16 # Reduced from 32
    kernLength2 = 15 # Keep same as build_eegnet
    x = SeparableConv2D(F2, (1, kernLength2), padding='same', use_bias=False, depthwise_regularizer=l2(l2_rate), pointwise_regularizer=l2(l2_rate))(x)
    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = AveragePooling2D((1, 8))(x)
    x = Dropout(dropout_rate)(x)
    # Classification Head - Reduced Dense Layer
    x = Flatten()(x)
    x = Dense(32, activation='elu', kernel_regularizer=l2(l2_rate))(x) # Reduced from 64
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs, name='build_q_eegnet')


input_shape =    X_tr_c.shape[1:]  # X_train.shape[1:]
model = build_q_eegnet(input_shape=input_shape, num_classes=NUM_CLASSES, dropout_rate=0.25, l2_rate=0.0001)
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# --- Logging Callback ---
class EpochLogger(Callback):
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        print(f"Epoch {epoch+1}: Train Acc={logs.get('accuracy', -1):.4f}, Val Acc={logs.get('val_accuracy', -1):.4f}")

# --- Save Best Model Callback ---
class SaveBestModel(Callback):
    def __init__(self, path='BCI_4_2a_best_model.keras'):
        super().__init__()
        self.best_val = -1
        self.path = path

    def on_epoch_end(self, epoch, logs=None):
        acc = logs.get('val_accuracy', 0)
        if acc > self.best_val:
            self.best_val = acc
            self.model.save(self.path)
            print(f"✔️ New best model saved with val_acc={acc:.4f}")

callbacks = [EpochLogger(), SaveBestModel()]

# --- Train ---

, , ,


model.fit(X_tr_c, y_tr_c,
          validation_data=(X_val_c, y_val_c),
          batch_size=64,
          epochs= 300,
          callbacks=callbacks,
          verbose=0,
          shuffle=True)

# --- Evaluate ---
val_preds = np.argmax(model.predict(X_val_c), axis=1)
val_true = np.argmax(y_val_c, axis=1)
acc = np.mean(val_preds == val_true)
print(f"Final Validation Accuracy: {acc:.4f}")

"""-- New best model saved with val_acc=0.7931

"""





#training_time = timing_callback.get_total_time() if 'timing_callback' in locals() else -1.0
epochs_ran = len(history.history['loss']) if history and 'loss' in history.history else 0
        #logging.error(f"An error occurred during training or evaluation: {fit_eval_error}", exc_info=True)
val_acc, val_loss = -1.0, -1.0; precision, recall, f1_score = -1.0, -1.0, -1.0
        #current_error = f"FitEvalError: {fit_eval_error}"

    # --- Plotting ---
if history and epochs_ran > 0: # Only plot if training actually happened
        logging.info("Generating training history plots...")
        #epoch_times_list = timing_callback.get_epoch_times()[:epochs_ran]
        # Pad with 0 for the start time at epoch 0
        #cumulative_time = np.cumsum([0] + epoch_times_list)
        epoch_axis = range(epochs_ran) # Epochs are 0 to epochs_ran-1

        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
#        fig.suptitle(f'Run {i+1}: {model_name}\n{config_str}', fontsize=9) # Slightly smaller font
        fig.suptitle(f'Run {0+1}: {model_name}\n', fontsize=9) # Slightly smaller font

        # Accuracy Plot
if 'accuracy' in history.history and 'val_accuracy' in history.history:
            axes[0].plot(epoch_axis, history.history['accuracy'], label='Train Acc')
            axes[0].plot(epoch_axis, history.history['val_accuracy'], label='Val Acc')
            axes[0].set_title('Accuracy vs. Epoch'); axes[0].set_xlabel('Epoch'); axes[0].legend(); axes[0].grid(True)
else: axes[0].set_title('Accuracy Plot Unavailable'); axes[0].text(0.5, 0.5, 'No Data', ha='center', va='center')

        # Loss Plot
if 'loss' in history.history and 'val_loss' in history.history:
            axes[1].plot(epoch_axis, history.history['loss'], label='Train Loss')
            axes[1].plot(epoch_axis, history.history['val_loss'], label='Val Loss')
            axes[1].set_title('Loss vs. Epoch'); axes[1].set_xlabel('Epoch'); axes[1].legend(); axes[1].grid(True)
else: axes[1].set_title('Loss Plot Unavailable'); axes[1].text(0.5, 0.5, 'No Data', ha='center', va='center')

        # Cumulative Time Plot
        # Need epochs_ran + 1 points for cumulative time plot (includes time=0 at epoch 0)
#---

plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap

plt.show()

plt.close(fig)







"""### try 1"""

import mne
import pandas as pd

def load_custom_set_and_events(set_file, csv_file, event_id_map, tmin=0.0, tmax=3.0):
    raw = mne.io.read_raw_eeglab(set_file, preload=True)
    sfreq = raw.info['sfreq']

    # Bandpass filter
    raw.filter(5., 35., fir_design='firwin')

    # Load events from CSV
    events_df = pd.read_csv(csv_file)  # Should have 'onset' and 'label'
    events = np.array([[int(row['onset']), 0, int(event_id_map[row['label']])] for _, row in events_df.iterrows()])

    epochs = mne.Epochs(raw, events, event_id=event_id_map, tmin=tmin, tmax=tmax, baseline=None, preload=True)
    X = epochs.get_data()[..., np.newaxis]
    y = np.array([event_id_map[label] for label in epochs.events[:, -1]])
    y_cat = tf.keras.utils.to_categorical(y, num_classes=len(event_id_map))

    return train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y)

set_file = "/kaggle/input/aad-eeg-data/SET files extracted from vhdr files/aadhar_bicepcurl_Left_10trials.set"
csv_file= "/kaggle/input/timestamps/label and timestamps csv/aadhar_bicepcurl_Left_10trials.csv"
d= load_custom_set_and_events(set_file, csv_file, event_id_map, tmin=0.0, tmax=3.0)
len(d)
